:scrollbar:
:data-uri:
:toc2:
:linkattrs:


= _Intelligence at the Edge_

:numbered:

== Architecture

image::docs/images/djl-object-detect-architecture.png[]

As depicted in the above diagram, the demo functionality is split into the following components:

. *Edge*
+
Containerized Quarkus/DJL app that runs on a edge device enabled with a webcam.
Periodically sends events via MQTT when state changes occur (ie:  DJL detects probability that # of detected objects has increased from 2 to 3).
. *Web*
+
MQTT broker deployed in OpenShift.  Broker has default MQTT port exposed via a _LoadBalancer_ service.  Also deployed to OpenShift is a Quarkus based app that subscribes to the MQTT broker topic and forwards those MQTT events to a web browser (as `Server Sent Events` (SSE)).

== DJL on _aarch64_

link:https://github.com/deepjavalibrary/djl/issues/375[DJL Arm Support]

== Edge app
This application can run in your local environment.

=== Pre-reqs

. *Webcam*
+
Should be mounted to your operating system at a path such as:  `/dev/video0`

. *GPU/CPU*
+
This app will auto-detect the presence of a GPU and utilize it if found.
Otherwise, the app will default to a CPU.
+
The app exposes various diagnostic related REST endpoints that provide insight as to the GPU/CPU it has found.

. *Network*
+
You'll need a reliable broadband network due to downloading of a large quantity of library dependencies.

. *JDK11* (or more recent)
+
ie: `sudo dnf install java-latest-openjdk-devel`

. *maven*
+
ie: `sudo dnf install maven`

. *cURL* (or similar http test utility)
+
ie: `sudo dnf install curl`

. *DJL_CACHE_DIR*
+
DJL engines link:https://djl.ai/docs/development/cache_management.html[download models] and any needed C++ shared object files to a directory specified by the environment variable: `DJL_CACHE_DIR` .  
You are encouraged to set this environment variable in your shell.  
Otherwise, DJL will write these files to: `$HOME/.djl.ai`

. *opencv-java*
+
ie: `sudo dnf install opencv-java`
+
NOTE: On Fedora/RHEL systems, opencv-java package places shared C++ object files in a path (/usr/lib/java) typically not included in `java.library.path`.   Subsequently, at runtime you'll need to specify a `java.library.path` that does include this directory.
+
NOTE: RPM packages for `opencv-java` appear to link:https://www.rpmfind.net/linux/rpm2html/search.php?query=opencv-java[only exist] for F36 / RHEL9  (or more recent)

.*gstreamer plugins*
+
ie: `sudo dnf install gstreamer1-plugin-libav gstreamer1-plugins-bad-free gstreamer1-plugins-good -y`
+
NOTE RPM packages for `gstreamer1-plugin-libav` appear to link:https://packages.fedoraproject.org/pkgs/gstreamer1-plugin-libav/gstreamer1-plugin-libav/[only exist] for F37 (or more recent)

=== Quarkus `dev mode`

The application can be run locally in quarkus `dev mode` (which enables live coding).

NOTE:  a GPU is not required to run the application.  If deep learning engine detects the presence of a GPU, then it will use it.  If not, the deep learning engine will default to CPUs.

. Seed the `maven wrapper`:
+
-----
$ mvn wrapper:wrapper
-----

. Run the application in quarkus `dev mode`` using any of the following deep learning engines:


.. *PyTorch*
+
-----
./mvnw quarkus:dev -Djvm.args=-Djava.library.path=/usr/lib/java -Ponnx
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`


=== Fedora 38 aarch64

link:https://github.com/deepjavalibrary/djl/issues/375#issuecomment-1200471807[DJL ARM support] is currently only offered for PyTorch and OnnxRuntime engines.



An alternative might be to install link:https://docs.djl.ai/engines/pytorch/pytorch-engine/index.html#load-your-own-pytorch-native-library[pytorch on the host] and specify the _PYTORCH_LIBRARY_PATH_ environment variable.

. Run `edge` app in `quarkus:dev` mode:
+
-----
$ ./mvnw clean quarkus:dev \
      -Djvm.args=-Djava.library.path=/usr/lib/java \
      -P onnx,pytorch-aarch64
-----

.. If running on a recent version of GLIBC (ie: in Fedora 38), you may need to replace the _libstdc++_ library that comes included with DJL's _2.0.1-20230709-cpu-precxx11-linux-aarch64_ package :
+
-----
$ ldd --version
$ strings /usr/lib64/libstdc++.so.6.0.31 | grep '^CXXABI_'
$ cp /usr/lib64/libstdc++.so.6.0.32 $HOME/.djl.ai/pytorch/2.0.1-20230709-cpu-precxx11-linux-aarch64/libstdc++.so.6
-----

.. Doing so will prevent the following exception:
+
-----
22:58:33 ERROR [io.qu.ru.Application] (main) Failed to start application (with profile prod): java.lang.UnsatisfiedLinkError: /tmp/opencv_openpnp10653577782654499938/nu/pattern/opencv/linux/ARMv8/libopencv_java470.so: /home/jbride/.djl.ai/pytorch/2.0.0-cpu-precxx11-linux-aarch64/libstdc++.so.6: version `CXXABI_1.3.8' not found (required by /tmp/opencv_openpnp10653577782654499938/nu/pattern/opencv/linux/ARMv8/libopencv_java470.so)
	at java.base/jdk.internal.loader.NativeLibraries.load(Native Method)
	at java.base/jdk.internal.loader.NativeLibraries$NativeLibraryImpl.open(NativeLibraries.java:388)
	at java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:232)
	at java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:174)
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2389)
	at java.base/java.lang.Runtime.load0(Runtime.java:755)
	at java.base/java.lang.System.load(System.java:1953)
	at nu.pattern.OpenCV$LocalLoader.<init>(OpenCV.java:330)
	at nu.pattern.OpenCV$LocalLoader.<init>(OpenCV.java:326)
	at nu.pattern.OpenCV$LocalLoader$Holder.<clinit>(OpenCV.java:336)
	at nu.pattern.OpenCV$LocalLoader.getInstance(OpenCV.java:340)
	at nu.pattern.OpenCV.loadLocally(OpenCV.java:323)
	at nu.pattern.OpenCV$SharedLoader.<init>(OpenCV.java:217)
	at nu.pattern.OpenCV$SharedLoader.<init>(OpenCV.java:189)
	at nu.pattern.OpenCV$SharedLoader$Holder.<clinit>(OpenCV.java:261)
	at nu.pattern.OpenCV$SharedLoader.getInstance(OpenCV.java:265)
	at nu.pattern.OpenCV.loadShared(OpenCV.java:183)
	at org.acme.apps.LiveObjectDetectionResource.startResource(LiveObjectDetectionResource.java:116)
-----

. Build `edge` app:
+
-----
$ mvn clean package -DskipTests -P onnx,pytorch-aarch64
-----

. Start `edge` app in JVM:
+
-----
$ java \
    -Djava.library.path=/usr/lib/java \
    -jar target/quarkus-app/quarkus-run.jar
-----


=== Linux Containers

==== Pre-reqs:

. *podman*
+
ie:  `dnf install podman`

. *quay.io*
+
Linux container images already exist in `quay.io`.
If you want to push to quay.io, then authenticate as follows:
+
-----
$ podman login quay.io
-----


. To support link:https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/configurations.md#djl-settings[off-line mode] of the DJL engines, a pre-seeded DJL cache will be mounted to the linux container.  
emporary directories and/or json files might be generated in this DJL cache.  
This DJL cache directory should be made writable by the container process for the following reasons:

.. Extraction of native C++ libraries included in DJL `fatjar` to $DJL_CACHE_DIR
.. Downloading of any models from DJL's ModelZoo that may be used by the application.

. Make $DJL_CACHE_DIR writable for container process:
+
-----
$ export DJL_CACHE_DIR_OCI=/u02/djl.ai.oci \
    && mkdir -p $DJL_CACHE_DIR_OCI

$ sudo semanage fcontext -a \
        -t container_file_t "$DJL_CACHE_DIR_OCI(/.*)?"

$ sudo restorecon -R $DJL_CACHE_DIR_OCI

$ podman unshare chown -R 185:185 $DJL_CACHE_DIR_OCI
-----

==== Create Linux Container

. Change directory into:  `djl-objectdetect`

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Build container and generate openshift/helm configs:
+
-----
$ ./mvnw clean package \
            -P$djl_engine \
            -Dquarkus.application.name=djl-objectdetect-$djl_engine \
            -DskipTests \
            -Dquarkus.container-image.build=true \
            -Dquarkus.container-image.push=true
-----

==== Execution

. Set an environment variable that specifies one of the possible deep learning engines:
+
-----
$ djl_engine=pytorch
-----
+
NOTE:  Possible options are:  *pytorch*, *mxnet*, or *tensorflow*

. Set environment variable indicating whether to run the DJL engine in offline mode:
+
-----
$ djl_offline=false
-----

. Specify the video card to capture from:
+
-----
$ djl_video=0
-----

. The `djl-objectdetection` container needs access to the host's video card.
+
Podman allows for that however you need to ensure that your host operating system user is a member of the `video` group:
+
-----
$ sudo usermod -a -G video <your OS user name>
-----
+
NOTE: For more info about podman's ability to provide access to the host machine's video card, please review link:https://www.redhat.com/sysadmin/files-devices-podman[this document].

. The `djl-objectdetection` container needs the ability to write video capture images to the filesystem of the host.
Subsequently, in this step you enable the local filesystem to be writable by the container:
+
-----
$ I_DIR=/tmp/org.acme.objectdetection/ \
    && mkdir -p $I_DIR

$ sudo semanage fcontext -a \
        -t container_file_t "$I_DIR(/.*)?"

$ sudo restorecon -R $I_DIR

$ podman unshare chown -R 185:185 $I_DIR
-----


. Run linux container using designated deep learning engine:
+
-----
$ podman run \
    --rm \
    --name djl-objectdetect-$djl_engine \
    -p 8080:8080 \
    -p 5005:5005 \
    -e JAVA_ENABLE_DEBUG="true" \
    -e JAVA_OPTS="-Dquarkus.http.host=0.0.0.0 -Djava.util.logging.manager=org.jboss.logmanager.LogManager -Doffline=$djl_offline -Dorg.acme.objectdetection.video.capture.device.id=$djl_video" \
    -e DJL_CACHE_DIR=/mnt/djl.ai \
    -v $DJL_CACHE_DIR_OCI:/mnt/djl.ai:z \
    --device /dev/video$djl_video \
    --group-add keep-groups \
    -v /tmp/org.acme.objectdetection:/tmp/org.acme.objectdetection:z \
    -v ./config/application.properties:/deployments/config/application.properties:z \
    quay.io/redhat_naps_da/djl-objectdetect-$djl_engine:0.0.3
-----

. View RESTful API exposed by app:
+
Open a browser tab and navigate to:  `localhost:8080/q/swagger-ui`


== Web app

=== Pre-reqs

. *OpenShift Container Platform*
.. Tested on OCP 4.13  (but earlier versions should also work fine as well)
.. CPU:
+
Plan for 500 millis

.. RAM:
+
Plan for 500Mb RAM

.. Storage:  no PVs needed

. *cURL* (or similar http test utility)
+
ie: `dnf install curl`

=== Build

. Build linux container image and push to quay.io:
+
NOTE:  execute the following from the root parent maven project dir.  ie: `flying-djl`
+
-----
$ mvn clean package -pl web -am \
      -DskipTests \
      -Dquarkus.container-image.build=true \
      -Dquarkus.container-image.push=true
-----

=== Deploy

. Create a ConfigMap from the project's _application.properties_:
+
-----
$ oc create cm djl-iclassification --from-file=config/application.properties
-----


. Determine node that pod landed on:
+
-----
$ oc get pod \
    -l deploymentconfig=djl-iclassification-pytorch \
    -o json \
    -n user1-services \
    | jq -r .items[0].spec.nodeName
-----
+
NOTE: The result should return the id of your GPU enabled node.

== Run Demo

. View mqtt message counts in Artemis admin console

.. Point your browser to the output of the following:
+
-----
$ http://localhost:8161
-----

.. Authenticate using the following credentials:  *djl*  /  *djl*
+
image::docs/images/djl-object-detect-mqtt-artemis-web-admin.png[]


. View video capture events in browser

.. Point your browser to the output of the following:
+
-----
$ echo -en "\nhttps://$(oc get route djl-objectdetect-web -n user1-djl --template='{{ .spec.host }}')/liveObject.html\n"
-----

== Model Diagnostics

=== Generate onnx default models

==== yolo5s

. Model sizes:

.. pytorch: 15MB
.. onnx: 28MB

. Clone `ultralytics` project and pull down all dependencies:
+
-----
(venv) $ git clone https://github.com/ultralytics/yolov5
(venv) $ (cd yolov5; git checkout v7.0; pip install -r requirements.txt)
-----

. Detect and predict:
+
-----
(venv) $ python yolov5/detect.py \
           --source https://github.com/redhat-na-ssa/flyingthings/blob/djl/flying-djl/docs/images/unAdulteredImage-1690148580.png
-----

. Export to onnx:
+
-----
$ python yolov5/export.py --weight=yolov5/yolov5s.pt --include=onnx
-----

==== yolo8n

. Model sizes:

.. pytorch:  6MB
.. onnx:  13MB

-----
$ deactivate
$ python3.9 -m venv ~/venv-yolo8
$ source ~/venv-yolo8/bin/activate
$ pip install -U ultralytics
$ cd %HOME/Downloads/ultralytics/
$ mkdir v8 && cd v8
$ yolo predict model=yolov8n \
    && yolo export model=yolov8n.pt format=onnx
$ zip -r yolov8n-onnx.zip serving.properties synset.txt yolov8n.onnx
-----

=== Prediction using Ultralytics Model Zoo

. Initial setup:
+
-----
$ python3.9 -m venv ~/venv
$ source ~/venv/bin/activate
(venv) $ pip install -U ultralytics
-----

. Predict using `yolo` executable and a model from its model zoo
+
-----
(venv) $  yolo predict \
              model=yolov8n.pt \
              source=flying-djl/docs/images/unAdulteredImage-1690148580.png \
              exist_ok=True


Ultralytics YOLOv8.0.141 ðŸš€ Python-3.11.4 torch-2.0.1+cu117 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)
YOLOv8n summary (fused): 168 layers, 3151904 parameters, 0 gradients

image 1/1 image 1/1 /u01/labs/mw/redhat-na-ssa/flyingthings/flying-djl/docs/images/unAdulteredImage-1690148580.png: 384x640 1 airplane, 54.5ms
Speed: 1.1ms preprocess, 54.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)
-----

. Predict using `yolo` executable and custom model:
+
-----
(venv) $ yolo predict \
            model=$HOME/Downloads/flyingthings/model_custom.pt \
            source=docs/images/unAdulteredImage-1690148580.png


Ultralytics YOLOv8.0.141 ðŸš€ Python-3.11.4 torch-2.0.1+cu117 CPU (11th Gen Intel Core(TM) i7-1185G7 3.00GHz)
Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs

image 1/1 /u01/labs/mw/redhat-na-ssa/flyingthings/flying-djl/docs/images/unAdulteredImage-1690148580.png: 384x640 1 Fixed Wing, 36.9ms
Speed: 1.6ms preprocess, 36.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)
-----


=== Detection using Ultralytics `detect.py`





. Detect using custom model:
+
-----
(venv) $ pip install dill
(venv) $ python yolov5/detect.py \
         --source https://github.com/redhat-na-ssa/flyingthings/blob/djl/flying-djl/docs/images/unAdulteredImage-1690148580.png \
         --weights ~/Downloads/flyingthings/model_custom.pt



Model summary (fused): 168 layers, 3006038 parameters, 0 gradients, 8.1 GFLOPs
Traceback (most recent call last):
  File "/u01/ai/ultralytics/yolov5/detect.py", line 261, in <module>
    main(opt)
  File "/u01/ai/ultralytics/yolov5/detect.py", line 256, in main
    run(**vars(opt))
  File "/home/jbride/venv/lib64/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/u01/ai/ultralytics/yolov5/detect.py", line 160, in run
    s += f"{n} {names[int(c)]}{'s' * (n > 1)}, "  # add to string
                ~~~~~^^^^^^^^
KeyError: 1994
-----

=== onnx

-----
(venv) $ python export.py --include=onnx
-----

=== aarch64 (Pi 4)

. video file processing requires appropriate gstreamer-plugins (webcam processing apparently does not)
. those gstreamer-plugins (along with opencv) are compiled using glibc++ libraries for F38
. DJL pytorch libraries (required when running onnxruntime engine) are compiled using an older version of glibc++
. Attempt to install torch libraries via pip and reference when DJL starts-up
. DJL JNI wrappers for triplet (DJL version, aarch64 cpu, pytorch version) doesn't exist

-----
$ ./mvnw clean quarkus:dev -Djvm.args=-Djava.library.path=/usr/lib/java -P onnx,pytorch-aarch64

Caused by: java.lang.UnsatisfiedLinkError: /usr/lib/java/libopencv_java470.so: /home/jbride/.djl.ai/pytorch/2.0.1-20230709-cpu-precxx11-linux-aarch64/libstdc++.so.6: version `GLIBCXX_3.4.29' not found (required by /usr/lib/java/libopencv_java470.so)
	at java.base/jdk.internal.loader.NativeLibraries.load(Native Method)
	at java.base/jdk.internal.loader.NativeLibraries$NativeLibraryImpl.open(NativeLibraries.java:388)
	at java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:232)
	at java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:174)
	at java.base/jdk.internal.loader.NativeLibraries.findFromPaths(NativeLibraries.java:315)
	at java.base/jdk.internal.loader.NativeLibraries.loadLibrary(NativeLibraries.java:287)
	at java.base/java.lang.ClassLoader.loadLibrary(ClassLoader.java:2422)
	at java.base/java.lang.Runtime.loadLibrary0(Runtime.java:818)
	at java.base/java.lang.System.loadLibrary(System.java:1989)
	at nu.pattern.OpenCV$SharedLoader.<init>(OpenCV.java:200)

$ strings /home/jbride/.djl.ai/pytorch/2.0.1-20230709-cpu-precxx11-linux-aarch64/libstdc++.so.6 | grep GLIBCXX

...
GLIBCXX_3.4.19


$ strings /usr/lib64/libstdc++.so.6 | grep GLIBCXX

...
GLIBCXX_3.4.32


-----

-----
$ sudo dnf install python39
$ python3.9 -m ensurepip --default-pip
$ python3.9 -m pip install torch
$ python3.9 -m pip show -f torch | grep Location
$ python3.9 -m pip show -f torch | grep Version
$ export PYTORCH_LIBRARY_PATH=/home/jbride/.local/lib/python3.9/site-packages/torch/lib \
    && export PYTORCH_VERSION=2.0.1 \
    && export PYTORCH_FLAVOR=cpu
$ (cd edge; ./mvnw clean quarkus:dev -Djvm.args=-Djava.library.path=/usr/lib/java -Ponnx)

Caused by: java.io.FileNotFoundException: https://publish.djl.ai/pytorch/2.0.1/jnilib/0.23.0/linux-aarch64/cpu/libdjl_torch.so
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1993)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1589)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:224)
	at java.base/java.net.URL.openStream(URL.java:1161)
	at ai.djl.util.Utils.openUrl(Utils.java:463)
	at ai.djl.util.Utils.openUrl(Utils.java:447)
	at ai.djl.pytorch.jni.LibUtils.downloadJniLib(LibUtils.java:509)
	... 44 more

$ ls -lt ~/.djl.ai/pytorch/2.0.1-20230709-cpu-precxx11-linux-aarch64/
0.23.0-libdjl_torch.so            libarm_compute_core-0793f69d.so   libc10.so                         libgomp-efb3da07.so.1.0.0         libtorch_cpu.so                   
libarm_compute-23619548.so        libarm_compute_graph-ebe58799.so  libgomp-d22c30c5.so.1.0.0         libstdc++.so.6                    libtorch.so

-----
